# Text Mining

## Overview
For this example, we will work with product review data from the following open science resource: [https://osf.io/tyue9](https://osf.io/tyue9)

Salminen, J., Kandpal, C., Kamel, A. M., Jung, S., & Jansen, B. J. (2022). Creating and detecting fake reviews of online products. Journal of Retailing and Consumer Services, 64, 102771. https://doi.org/10.1016/j.jretconser.2021.102771

## Load Libs
```{r}
library(readr)
library(tidyverse)
library(tidytext)
library(textdata)
library(topicmodels)
library(wordcloud)
library(ggwordcloud)
```

## Load full dataset
``` {r}
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

df <- readr::read_csv("data/fake reviews dataset.csv") %>%
  mutate(id = row_number()) %>% # add row id
  select(id, category, everything())

```

## Produce a data quality report
```{r}
df_dq <- skimr::skim(df)

# get proportion of stop_words by lexicon ----
prop.table(table(stop_words$lexicon))

# if a word you cared about is in the stop word dictionary you will lose it
# in your analysis. Use the code below if you need to exclude words from the stopword dictionary.
# stop_words_filt = stop_words %>%
#   filter(word != "better")

# Otherwise continue
stop_words_filt = stop_words

```

## Tokenize the text
```{r}

# text mining analyses ----
section_freeresponse_tokens = df %>%
  unnest_tokens(bigram,
                "text_",
                token = "ngrams",
                n = 2,
                drop = F) %>%
  separate(bigram, c("word1", "word2"), sep = " ", remove=F) %>%
  filter(!word1 %in% stop_words_filt$word) %>%
  filter(!word2 %in% stop_words_filt$word)

```


## Compute [td-idf statistic](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)

`tf_idf` = statistic intended to reflect how important word is to a document

```{r}

# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

section_fr_1_tf_idf = section_freeresponse_tokens %>%
  count(category, word1) %>%
  bind_tf_idf(word1, category, n) %>%
  arrange(desc(tf_idf))

unique(df$category)

# vis tf_idf ----
section_fr_1_tf_idf %>%
  group_by(category) %>%
  slice_max(tf_idf, n = 2) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word1, tf_idf), fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

## Generate a wordcloud
```{r}
# https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
wordcloud::wordcloud(words = section_fr_1_tf_idf %>% pull(word1),
                     freq = section_fr_1_tf_idf %>% pull(n),
                     min.freq = 300,
                     max.words=200,
                     random.order=FALSE,
                     rot.per=0.1)

```

## Conduct simple sentiment analysis
```

# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# sentiment analysis ----
AFINN <- get_sentiments("afinn")

# more options ---
# get_sentiments(lexicon = c("bing", "afinn", "loughran", "nrc"))

# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# merge sentiment with dataset ----
sent_words <- section_freeresponse_tokens %>%
  inner_join(AFINN, by = c(word1 = "word"))

sent_words_fj <- section_freeresponse_tokens %>%
  full_join(AFINN, by = c(word1 = "word"))

# produce various aggregates of sentiment ----
count_words_by_sent = sent_words %>%
  count(category, value, sort = TRUE) %>%
  mutate(n_cut = cut(n, c(0,500,3000,Inf)))

```

## Visualize sentiment

```
ggplot(count_words_by_sent, aes(value, n_cut)) +
  geom_tile() +
  facet_grid(.~category)

avg_sent_by_category = sent_words %>%
 group_by(category) %>%
  summarise(avg_sent = mean(value, na.rm=T),
            sd_sent = sd(value, na.rm=T))

mv = ggplot(avg_sent_by_category, aes(category, avg_sent)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle=90))

sv = ggplot(avg_sent_by_category, aes(category, sd_sent)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle=90))

cowplot::plot_grid(mv, sv, ncol=2)

```

---

## Topic modeling analysis
Coming soon!
Resource: https://www.tidytextmining.com/topicmodeling.html

```{r}

# ap_data = section_freeresponse_tokens %>%
#   cast_dtm(text_, word1, n)
# 
# ap_lda <- LDA(ap_data, k = 2, control = list(seed = 1234))
# ap_topics <- tidy(ap_lda, matrix = "beta")
# ap_documents <- tidy(ap_lda, matrix = "gamma")
# 
# ap_top_terms <- ap_topics %>%
#   filter(!is.na(term)) %>%
#   group_by(topic) %>%
#   slice_max(beta, n = 20) %>%
#   ungroup() %>%
#   arrange(topic, -beta)
# 
# ap_top_terms %>%
#   mutate(term = reorder_within(term, beta, topic)) %>%
#   ggplot(aes(beta, term, fill = factor(topic))) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~ topic, scales = "free") +
#   scale_y_reordered()
# 
# beta_wide <- ap_topics %>%
#   mutate(topic = paste0("topic", topic)) %>%
#   pivot_wider(names_from = topic, values_from = beta) %>%
#   filter(topic1 > .001 | topic2 > .001) %>%
#   mutate(log_ratio_t2_t1 = log2(topic2 / topic1),
#          log_ratio_t3_t1 = log2(topic3 / topic1))
# 
# ggplot(beta_wide, aes(log_ratio_t2_t1, term)) +
#   geom_bar(stat="identity")
```
